{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc25b628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from opencv-python) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219bc094",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5802b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d025d7",
   "metadata": {},
   "source": [
    "# Loading YOLO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1aec95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa8afd",
   "metadata": {},
   "source": [
    "# Loading Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2170930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "classes = []\n",
    "with open('coco.names', 'r') as f:\n",
    "    classes= f.read().splitlines()\n",
    "    \n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45620f1e",
   "metadata": {},
   "source": [
    "# Reading Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fa1d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('office2.jpg')\n",
    "height, width, _ = img.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a880de3",
   "metadata": {},
   "source": [
    "# Creating Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50a800a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. img:\\nThis is the input image that you want to process.\\n\\n2. 1/255:\\nThis is a scaling factor applied to each pixel value of the image. It is common to scale pixel values to the range [0, 1] \\nfor neural network input. In this case, each pixel value is divided by 255.\\n\\n3. (416, 416):\\nThis is the spatial size to which the input image will be resized. The YOLO model used in this code expects input images \\nto be of size 416x416 pixels.\\n\\n4. (0, 0, 0):\\nThis represents the mean subtraction values for each channel. Subtracting the mean helps center the data around zero. \\nIn this case, (0, 0, 0) means no mean subtraction.\\n\\n5. swapRB=True:\\nThis parameter specifies whether to swap the Red and Blue channels in the input image. \\nOpenCV loads images in BGR (Blue, Green, Red) order by default, but many pre-trained models, including YOLO, \\nexpect images in RGB order. Setting swapRB=True swaps the channels accordingly.\\n\\n6. crop=False:\\nThis parameter determines whether to crop the image after resizing it to the specified size. \\nIn this case, crop=False means no cropping is performed.\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = cv2.dnn.blobFromImage(img, 1/255, (416,416), (0,0,0), swapRB=True, crop=False)\n",
    "'''\n",
    "1. img:\n",
    "This is the input image that you want to process.\n",
    "\n",
    "2. 1/255:\n",
    "This is a scaling factor applied to each pixel value of the image. It is common to scale pixel values to the range [0, 1] \n",
    "for neural network input. In this case, each pixel value is divided by 255.\n",
    "\n",
    "3. (416, 416):\n",
    "This is the spatial size to which the input image will be resized. The YOLO model used in this code expects input images \n",
    "to be of size 416x416 pixels.\n",
    "\n",
    "4. (0, 0, 0):\n",
    "This represents the mean subtraction values for each channel. Subtracting the mean helps center the data around zero. \n",
    "In this case, (0, 0, 0) means no mean subtraction.\n",
    "\n",
    "5. swapRB=True:\n",
    "This parameter specifies whether to swap the Red and Blue channels in the input image. \n",
    "OpenCV loads images in BGR (Blue, Green, Red) order by default, but many pre-trained models, including YOLO, \n",
    "expect images in RGB order. Setting swapRB=True swaps the channels accordingly.\n",
    "\n",
    "6. crop=False:\n",
    "This parameter determines whether to crop the image after resizing it to the specified size. \n",
    "In this case, crop=False means no cropping is performed.\n",
    "'''\n",
    "\n",
    "#for b in blob:\n",
    "#    for n,img_blob in enumerate(b):\n",
    "#        cv2.imshow(str(n), img_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a34dbad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[10  0  3  9  1 11  7]\n"
     ]
    }
   ],
   "source": [
    "net.setInput(blob) #sets the blob as the input to the YOLO network.\n",
    "\n",
    "\n",
    "output_layers_names = net.getUnconnectedOutLayersNames()\n",
    "layerOutputs = net.forward(output_layers_names) #performs a forward pass through the YOLO network to obtain the output layers.\n",
    "\n",
    "\n",
    "boxes = []\n",
    "confidences = []\n",
    "class_ids = []\n",
    "\n",
    "for output in layerOutputs:\n",
    "    for detection in output:\n",
    "        # Processing each detection, extracting information,\n",
    "        # and storing relevant data in lists.\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            \n",
    "            x = int(center_x - w/2)\n",
    "            y = int(center_y - h/2)\n",
    "            \n",
    "            boxes.append([x,y,w,h])\n",
    "            confidences.append((float(confidence)))\n",
    "            class_ids.append(class_id)\n",
    "            \n",
    "            '''processes the output of the YOLO network, extracting bounding boxes, confidence scores, \n",
    "            and class IDs for detections with confidence greater than 0.5.'''\n",
    "            \n",
    "            \n",
    "print(len(boxes))\n",
    "indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4) #applies Non-Maximum Suppression to filter out overlapping \n",
    "                                                        #and low-confidence bounding boxes. The result is stored in the indexes variable.\n",
    "print(indexes.flatten())\n",
    "\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "colors = np.random.uniform(0, 155, size=(len(boxes), 3))\n",
    "\n",
    "for i in indexes.flatten():\n",
    "    # Extracting box coordinates, label, confidence, and color.\n",
    "    # Drawing rectangle and text on the image.\n",
    "\n",
    "    x,y,w,h = boxes[i]\n",
    "    label = str(classes[class_ids[i]])\n",
    "    confidence = str(round(confidences[i], 2))\n",
    "    color = colors[i]\n",
    "    cv2.rectangle(img, (x,y), (x+w, y+h), color, 2)\n",
    "    cv2.putText(img, label + \" \" + confidence, (x, y+20), font,2, (255,255,255), 2)\n",
    "    \n",
    "    \n",
    "\n",
    "cv2.imshow('Image', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5cd9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcb7548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d4a339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
